<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CMU 15645|Data Storage(1)]]></title>
    <url>%2F2019%2F12%2F26%2FCMU%2015645%3A%20Data%20Storage(1)%2F</url>
    <content type="text"><![CDATA[1 存储CMU 15645课程要求实现一个面向磁盘(disk-oriented)的DBMS，因此假定数据存放在非易失性的磁盘中。 根据存储介质的易失性和非易失性，将存储层次划分为以下两种类别： 易失性设备：易失性指当机器断电时，存储的数据也随之丢失。而此类设备往往支持按字节寻址的快速随机访问，程序可以根据字节地址取出其中的数据。这里通常用它指代内存。（这并不代表易失性设备只有内存，其他如寄存器、CPU cache等） 非易失性设备：不必持续供电来存储数据（即数据不会因为断电而丢失），通常是按块/页寻址的。因此想要读入某处地址的数据，就必须同时读取所在的整块/页的数据。通常是顺序访问的（即不可以随机访问）。这里通常用它指代磁盘。 由于数据存储在磁盘上，DBMS就必需负责完成数据在磁盘和内存之间的交换工作。 2 面向磁盘的DBMS概括数据库文件中的数据按页组织，第一页是页目录(directory page)。为了对数据进行操作，DBMS通过缓存池(buffer pool)将数据存到内存中，缓存池管理数据在内存和磁盘之间的换入换出。DBMS通过执行引擎(execution engine)来进行查询操作，查询引擎向缓存池请求特定的页，缓存池则将该页换入内存，并返回一个指向该页的指针，同时缓存池也要确保查询引擎操作的页已经存放在内存中。 3 DBMS 与 OS 的对比DBMS的设计目标之一就是保证数据库的大小超过可用内存的大小。由于磁盘的读写开销较大，因此DBMS需要在从磁盘在获取数据时能够处理其他的查询操作。 为了实现这一目标，可以利用mmap(内存映射)实现虚拟内存，将文件的内容映射到进程的地址空间中。此时OS负责页的换入和换出，但是如果mmap发生了页故障，就会阻塞整个进程。因此比起OS，更应该由DBMS自己来控制流程，它比OS更清楚有关的数据访问和查询操作。 但是可以利用OS进行如下操作： madvise: 通知OS准备读某些特定的页。 mlock: 通知OS不要将内存换出到磁盘。 msync: 通知OS将内存内容刷新到磁盘上。 虽然OS可以提供某些DBMS需要的功能，但是由DBMS本身实现相关的过程可以使DBMS具有更好的控制能力和性能表现。 4 数据库页(Database Pages)​]]></content>
      <categories>
        <category>Database System</category>
      </categories>
      <tags>
        <tag>CMU 15645</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读Paper|DeepFool算法简介]]></title>
    <url>%2F2019%2F08%2F27%2F%E8%AF%BBPaper-DeepFool%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[概述​ DeepFool也是一种基于梯度的白盒攻击算法（与FGSM类似），由Seyed-Mohsen Moosavi-Dezfooli等人在DeepFool: a simple and accurate method to fool deep neural networks 一文中提出。DeepFool算法不用指定学习速率$\varepsilon$，可以计算出比FGSM算法更小的扰动来达到攻击的目的。 算法简介​ 论文中分别提出了针对二分类和多分类的DeepFool算法，并在不同的模型和数据集上进行了实验。 二分类问题​ 下图为论文中摘取的二分类问题示例，为了改变分类器的决策，在图片上叠加的最小扰动就是$x_{0}$到$f(x)$垂直方向的距离$r_{*}(x)$。 ​ ​ $r_{*}(x)$的解析解可用下式表示. 根据下式，可以很容易地计算得到 $r_{*}(x)$，但是这个扰动值只能使样本达到分类面，而不足以越过， 故最终的扰动值为 $r_{*}(1+\eta)$，$\eta\ll1$，实验中一般取0.02。 ​ ​ 算法伪代码描述如下： 多分类问题​ 下图为多分类器的示意图，图中实线表示分类器真实的分类超平面，而虚线则代表近似的线性分类超平面，在每次迭代过程中，总是基于当前的迭代值，计算一组近似的线性分类超平面，并根据这组近似超平面，计算扰动，并进行迭代得到下一次的迭代值。 ​ 算法伪代码描述如下（移动距离比较小，参数矩阵可以用梯度代替）： 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485def deepfool_attack(sess, x, predictions, logits, grads, sample, nb_candidate, overshoot, max_iter, clip_min, clip_max, feed=None): """ TensorFlow implementation of DeepFool. Paper link: see https://arxiv.org/pdf/1511.04599.pdf :param sess: TF session :param x: The input placeholder :param predictions: The model's sorted symbolic output of logits, only the top nb_candidate classes are contained :param logits: The model's unnormalized output tensor (the input to the softmax layer) :param grads: Symbolic gradients of the top nb_candidate classes, procuded from gradient_graph :param sample: Numpy array with sample input :param nb_candidate: The number of classes to test against, i.e., deepfool only consider nb_candidate classes when attacking(thus accelerate speed). The nb_candidate classes are chosen according to the prediction confidence during implementation. :param overshoot: A termination criterion to prevent vanishing updates :param max_iter: Maximum number of iteration for DeepFool :param clip_min: Minimum value for components of the example returned :param clip_max: Maximum value for components of the example returned :return: Adversarial examples """ adv_x = copy.copy(sample) # Initialize the loop variables iteration = 0 current = utils_tf.model_argmax(sess, x, logits, adv_x, feed=feed) if current.shape == (): current = np.array([current]) w = np.squeeze(np.zeros(sample.shape[1:])) # same shape as original image r_tot = np.zeros(sample.shape) original = current # use original label as the reference _logger.debug( "Starting DeepFool attack up to %s iterations", max_iter) # Repeat this main loop until we have achieved misclassification while (np.any(current == original) and iteration &lt; max_iter): if iteration % 5 == 0 and iteration &gt; 0: _logger.info("Attack result at iteration %s is %s", iteration, current) gradients = sess.run(grads, feed_dict=&#123;x: adv_x&#125;) predictions_val = sess.run(predictions, feed_dict=&#123;x: adv_x&#125;) for idx in range(sample.shape[0]): pert = np.inf if current[idx] != original[idx]: continue for k in range(1, nb_candidate): w_k = gradients[idx, k, ...] - gradients[idx, 0, ...] f_k = predictions_val[idx, k] - predictions_val[idx, 0] # adding value 0.00001 to prevent f_k = 0 pert_k = (abs(f_k) + 0.00001) / np.linalg.norm(w_k.flatten()) if pert_k &lt; pert: pert = pert_k w = w_k r_i = pert * w / np.linalg.norm(w) r_tot[idx, ...] = r_tot[idx, ...] + r_i adv_x = np.clip(r_tot + sample, clip_min, clip_max) current = utils_tf.model_argmax(sess, x, logits, adv_x, feed=feed) if current.shape == (): current = np.array([current]) # Update loop variables iteration = iteration + 1 # need more revision, including info like how many succeed _logger.info("Attack result at iteration %s is %s", iteration, current) _logger.info("%s out of %s become adversarial examples at iteration %s", sum(current != original), sample.shape[0], iteration) # need to clip this image into the given range adv_x = np.clip((1 + overshoot) * r_tot + sample, clip_min, clip_max) return adv_x ​ 更详细的代码可见看这里。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>对抗样本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读Paper|FGSM算法简介]]></title>
    <url>%2F2019%2F08%2F21%2F%E8%AF%BBPaper-FGSM%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[概述FGSM（Fast Gradient Sign Method）算法是一种白盒攻击算法，是Ian J. Goodfellow在Explaining and harnessing adversarial examples一文中提出的。论文中提出，可利用该方法作为一种正则化的手段，从而提高神经网络的准确性，同时增加抵抗对抗攻击的能力。 算法原理例子​ 文章首先解释了线性模型中的对抗样本，针对输入$x$，令扰动输入为$\tilde{x}=x+\eta$，其中$\eta$为很小的变化且$||\eta||_{\infty}&lt;\epsilon$，考虑到$$\mathbf{\omega}^\mathrm{T}\tilde{x}=\mathbf{\omega}^\mathrm{T}x+\mathbf{\omega}^\mathrm{T}\eta.$$$\mathbf{\omega}$为权重向量，对抗扰动通过附加$\mathbf{\omega}^\mathrm{T}\eta$项，从而使激活函数的值而发生变化。因此如果令$\eta=sign(\omega)$（为了保证变化量与梯度方向一致），将每一个维度的微小影响叠加起来，就可以对最终分类结果产生较大的影响。 ​ 下图是一个经典的对抗样本示例，在ImageNet上训练后的GoogLeNet将原始图片以57.7%的概率识别为熊猫，而在图片上叠加一些扰动后，网络将样本识别以99.3%的概率识别为长臂猿，然而在人眼看来两幅图片几乎没有差别。 算法描述​ 令$\theta$为模型参数，$x$为模型输入，$y$为对应的目标，$J(\theta,x,y)$为用来训练神经网络的损失函数：$$\eta=\epsilon \text{sign}(\nabla_{x}J(\theta,x,y)).$$​ 简单的讲就是求出损失函数对于输入$x$的梯度，将上式计算出扰动附加到输入$x$上，使$x$朝梯度上升的方向移动，从而使分类结果偏离样本的原始标签。 算法实现算法的tensorflow实现参考了cleverhans，详细可以参照原github。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283def fgm(x, logits, y=None, eps=0.3, ord=np.inf, clip_min=None, clip_max=None, clip_grad=False, targeted=False, sanity_checks=True): """ TensorFlow implementation of the Fast Gradient Method. :param x: the input placeholder :param logits: output of model.get_logits :param y: (optional) A placeholder for the true labels. If targeted is true, then provide the target label. Otherwise, only provide this parameter if you'd like to use true labels when crafting adversarial samples. Otherwise, model predictions are used as labels to avoid the "label leaking" effect (explained in this paper: https://arxiv.org/abs/1611.01236). Default is None. Labels should be one-hot-encoded. :param eps: the epsilon (input variation parameter) :param ord: (optional) Order of the norm (mimics NumPy). Possible values: np.inf, 1 or 2. :param clip_min: Minimum float value for adversarial example components :param clip_max: Maximum float value for adversarial example components :param clip_grad: (optional bool) Ignore gradient components at positions where the input is already at the boundary of the domain, and the update step will get clipped out. :param targeted: Is the attack targeted or untargeted? Untargeted, the default, will try to make the label incorrect. Targeted will instead try to move in the direction of being more like y. :return: a tensor for the adversarial example """ asserts = [] # If a data range was specified, check that the input was in that range if clip_min is not None: asserts.append(utils_tf.assert_greater_equal( x, tf.cast(clip_min, x.dtype))) if clip_max is not None: asserts.append(utils_tf.assert_less_equal(x, tf.cast(clip_max, x.dtype))) # Make sure the caller has not passed probs by accident assert logits.op.type != 'Softmax' if y is None: # Using model predictions as ground truth to avoid label leaking preds_max = reduce_max(logits, 1, keepdims=True) y = tf.to_float(tf.equal(logits, preds_max)) y = tf.stop_gradient(y) y = y / reduce_sum(y, 1, keepdims=True) # Compute loss loss = softmax_cross_entropy_with_logits(labels=y, logits=logits) if targeted: loss = -loss # Define gradient of loss wrt input grad, = tf.gradients(loss, x) if clip_grad: grad = utils_tf.zero_out_clipped_grads(grad, x, clip_min, clip_max) optimal_perturbation = optimize_linear(grad, eps, ord) # Add perturbation to original example to obtain adversarial example adv_x = x + optimal_perturbation # If clipping is needed, reset all values outside of [clip_min, clip_max] if (clip_min is not None) or (clip_max is not None): # We don't currently support one-sided clipping assert clip_min is not None and clip_max is not None adv_x = utils_tf.clip_by_value(adv_x, clip_min, clip_max) if sanity_checks: with tf.control_dependencies(asserts): adv_x = tf.identity(adv_x) return adv_x ​ 简要理解：代码中用targeted变量来标明是否为定向攻击，若取值为False，则会使样本预测值朝梯度上升的方向移动，加大与给定标签logits之间的偏移程度，反之则靠近标签logits。接下来计算样本x与损失函数loss之间的梯度，然后通过optimize_linear函数将偏移量叠加到输入上。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>对抗样本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗机器学习(Adversarial Machine Learning)相关的论文合集]]></title>
    <url>%2F2019%2F08%2F20%2F%E5%AF%B9%E6%8A%97%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Adversarial-Machine-Learning-%E7%9B%B8%E5%85%B3%E7%9A%84%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[预备/基础 Intriguing properties of neural networks Evasion Attacks against Machine Learning at Test Time Explaining and Harnessing Adversarial Examples (FGSM/FGM算法) 攻击 The Limitations of Deep Learning in Adversarial Settings DeepFool: a simple and accurate method to fool deep neural networks (DeepFool算法) Towards Evaluating the Robustness of Neural Networks 迁移性 Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples Delving into Transferable Adversarial Examples and Black-box Attacks Universal adversarial perturbations 对抗样本检测 On Detecting Adversarial Perturbations Detecting Adversarial Samples from Artifacts Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods 有限威胁模型攻击 ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors 物理攻击 Adversarial examples in the physical world Synthesizing Robust Adversarial Examples Robust Physical-World Attacks on Deep Learning Models 验证 Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks Certifying Some Distributional Robustness with Principled Adversarial Training 防御与攻击 MagNet: a Two-Pronged Defense against Adversarial Examples MagNet and “Efficient Defenses Against Adversarial Attacks” are Not Robust to Adversarial Examples Towards Deep Learning Models Resistant to Adversarial Attacks Attacking the Madry Defense Model with L1-based Adversarial Examples Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples Adversarial Risk and the Dangers of Evaluating Against Weak Attacks Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training Towards the first adversarially robust neural network model on MNIST Adversarial Attacks on Neural Network Policies Audio Adversarial Examples: Targeted Attacks on Speech-to-Text Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples Adversarial examples for generative models 更全面的关于对抗机器学习的文献列表，可以参见这篇博客。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
</search>
