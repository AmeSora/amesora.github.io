<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[读Paper|FGSM算法简介]]></title>
    <url>%2F2019%2F08%2F21%2F%E8%AF%BBPaper-FGSM%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[概述FGSM（Fast Gradient Sign Method）算法是一种白盒攻击算法，是Ian J. Goodfellow在Explaining and harnessing adversarial examples一文中提出的。论文中提出，可利用该方法作为一种正则化的手段，从而提高神经网络的准确性，同时增加抵抗对抗攻击的能力。 算法原理例子文章首先解释了线性模型中的对抗样本，针对输入$x$，令扰动输入为$\tilde{x}=x+\eta$，其中$\eta$为很小的变化且$||\eta||_{\infty}&lt;\epsilon$，考虑到$$\mathbf{\omega}^\mathrm{T}\tilde{x}=\mathbf{\omega}^\mathrm{T}x+\mathbf{\omega}^\mathrm{T}\eta.$$$\mathbf{\omega}$为权重向量，对抗扰动通过附加$\mathbf{\omega}^\mathrm{T}\eta$项，从而使激活函数的值而发生变化。因此如果令$\eta=sign(\omega)$（为了保证变化量与梯度方向一致），将每一个维度的微小影响叠加起来，就可以对最终分类结果产生较大的影响。 算法描述令$\theta$为模型参数，$x$为模型输入，$y$为对应的目标，$J(\theta,x,y)$为用来训练神经网络的损失函数：$$\eta=\epsilon \text{sign}(\nabla_{x}J(\theta,x,y)).$$简单的讲就是求出损失函数对于输入$x$的梯度，将上式计算出扰动附加到输入$x$上，使$x$朝梯度上升的方向移动，从而使分类结果偏离样本的原始标签。 算法实现算法的tensorflow实现参考了cleverhans，详细可以参照原github。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283def fgm(x, logits, y=None, eps=0.3, ord=np.inf, clip_min=None, clip_max=None, clip_grad=False, targeted=False, sanity_checks=True): """ TensorFlow implementation of the Fast Gradient Method. :param x: the input placeholder :param logits: output of model.get_logits :param y: (optional) A placeholder for the true labels. If targeted is true, then provide the target label. Otherwise, only provide this parameter if you'd like to use true labels when crafting adversarial samples. Otherwise, model predictions are used as labels to avoid the "label leaking" effect (explained in this paper: https://arxiv.org/abs/1611.01236). Default is None. Labels should be one-hot-encoded. :param eps: the epsilon (input variation parameter) :param ord: (optional) Order of the norm (mimics NumPy). Possible values: np.inf, 1 or 2. :param clip_min: Minimum float value for adversarial example components :param clip_max: Maximum float value for adversarial example components :param clip_grad: (optional bool) Ignore gradient components at positions where the input is already at the boundary of the domain, and the update step will get clipped out. :param targeted: Is the attack targeted or untargeted? Untargeted, the default, will try to make the label incorrect. Targeted will instead try to move in the direction of being more like y. :return: a tensor for the adversarial example """ asserts = [] # If a data range was specified, check that the input was in that range if clip_min is not None: asserts.append(utils_tf.assert_greater_equal( x, tf.cast(clip_min, x.dtype))) if clip_max is not None: asserts.append(utils_tf.assert_less_equal(x, tf.cast(clip_max, x.dtype))) # Make sure the caller has not passed probs by accident assert logits.op.type != 'Softmax' if y is None: # Using model predictions as ground truth to avoid label leaking preds_max = reduce_max(logits, 1, keepdims=True) y = tf.to_float(tf.equal(logits, preds_max)) y = tf.stop_gradient(y) y = y / reduce_sum(y, 1, keepdims=True) # Compute loss loss = softmax_cross_entropy_with_logits(labels=y, logits=logits) if targeted: loss = -loss # Define gradient of loss wrt input grad, = tf.gradients(loss, x) if clip_grad: grad = utils_tf.zero_out_clipped_grads(grad, x, clip_min, clip_max) optimal_perturbation = optimize_linear(grad, eps, ord) # Add perturbation to original example to obtain adversarial example adv_x = x + optimal_perturbation # If clipping is needed, reset all values outside of [clip_min, clip_max] if (clip_min is not None) or (clip_max is not None): # We don't currently support one-sided clipping assert clip_min is not None and clip_max is not None adv_x = utils_tf.clip_by_value(adv_x, clip_min, clip_max) if sanity_checks: with tf.control_dependencies(asserts): adv_x = tf.identity(adv_x) return adv_x]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>对抗样本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗机器学习(Adversarial Machine Learning)相关的论文合集]]></title>
    <url>%2F2019%2F08%2F20%2F%E5%AF%B9%E6%8A%97%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Adversarial-Machine-Learning-%E7%9B%B8%E5%85%B3%E7%9A%84%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[预备/基础 Intriguing properties of neural networks Evasion Attacks against Machine Learning at Test Time Explaining and Harnessing Adversarial Examples (FGSM/FGM算法) 攻击 The Limitations of Deep Learning in Adversarial Settings DeepFool: a simple and accurate method to fool deep neural networks (DeepFool算法) Towards Evaluating the Robustness of Neural Networks 迁移性 Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples Delving into Transferable Adversarial Examples and Black-box Attacks Universal adversarial perturbations 对抗样本检测 On Detecting Adversarial Perturbations Detecting Adversarial Samples from Artifacts Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods 有限威胁模型攻击 ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors 物理攻击 Adversarial examples in the physical world Synthesizing Robust Adversarial Examples Robust Physical-World Attacks on Deep Learning Models 验证 Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks Certifying Some Distributional Robustness with Principled Adversarial Training 防御与攻击 MagNet: a Two-Pronged Defense against Adversarial Examples MagNet and “Efficient Defenses Against Adversarial Attacks” are Not Robust to Adversarial Examples Towards Deep Learning Models Resistant to Adversarial Attacks Attacking the Madry Defense Model with L1-based Adversarial Examples Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples Adversarial Risk and the Dangers of Evaluating Against Weak Attacks Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training Towards the first adversarially robust neural network model on MNIST Adversarial Attacks on Neural Network Policies Audio Adversarial Examples: Targeted Attacks on Speech-to-Text Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples Adversarial examples for generative models 更全面的关于对抗机器学习的文献列表，可以参见这篇博客。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
</search>
